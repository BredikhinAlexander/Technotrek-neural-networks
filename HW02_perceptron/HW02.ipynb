{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmymaLV4yMDx"
   },
   "source": [
    "# ДЗ №2 - обучение модели трехслойного перцептрона методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n5WGg3lyMDz"
   },
   "source": [
    "## (1)\n",
    "В качестве теоретического задания в этом ДЗ предлагается провести вывод функции ошибки для задачи регрессии в предположении, что целевая переменная подчиняется распределению Лапласа. Также предлагается воспользоваться байесовским выводом и в том же предположении относительно распределения целевой переменной вывести форму функции потерь с условием лапласовского априорного распределения параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем формулу Байеса: \n",
    "$$ P(\\theta | X) = \\frac{P \\left( X| \\theta \\right) P(\\theta)}{P(X)} $$\n",
    "тут $ P(X| \\theta) $ - правдоподобие (likelihood)\n",
    "\n",
    "$ P(\\theta)$ - prior\n",
    "\n",
    "$ P(X) $ - evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем, что решаем задачу регрессии и предполагаем, что целевая переменная подчиняется распределению Лапласа: $$\n",
    "g(x)=\\frac{\\alpha}{2} e^{-\\alpha|x-\\beta|}, \\quad-\\infty<x<+\\infty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = \\theta^T X + g(x)$$\n",
    "\n",
    "Для простоты считаем, что $ \\beta = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(t| X, \\theta, \\alpha) = \\frac{\\alpha}{2} e^{-\\alpha|t - \\theta^T X| } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^* = \\mathbf{argmax} \\prod\\limits_{i = 1}^{n} \\left( \\frac{\\alpha}{2} e^{-\\alpha|t - \\theta^T X| } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "логарифмируем, чтобы перейти к сумме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^* = n \\ln \\frac{\\alpha}{2} +  \\ln \\prod\\limits_{i = 1}^{n} \\left(  e^{-\\alpha|t - \\theta^T X| } \\right) = n \\ln \\frac{\\alpha}{2} + \\sum\\limits_{i = 1}^{n} \\ln \\left(  e^{-\\alpha|t - \\theta^T X| } \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = n \\ln \\frac{\\alpha}{2} - \\alpha \\sum\\limits_{i = 1}^{n} |t - \\theta^T X|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили известную функцию ошибки MAE (минимизируем сумму модулей: $ \\sum\\limits_{i = 1}^{n} |t - \\theta^T X| $ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для Байевского вывода предполагаем, что целевая переменная и параметры распределены по Лапласу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(\\theta | t) = P(t | X, \\theta, \\alpha) P(\\theta) $$ (не учитываем evidence, так как он не зависит от параметров нашей модели)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова логарифмируем, получаем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\ln P(\\theta | t) = \\frac{1}{\\alpha} \\sum\\limits_{i = 1}^{n} |t - \\theta^T X| + \\frac{\\alpha}{2} |\\theta| + const   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в таких задачах (когда и целевая переменная и параметры распределены по Лапласу) стоит использовать L1 регуляризацию "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBWH5p9dyMD1"
   },
   "source": [
    "## (2)\n",
    "В этом ДЗ предлагается реализовать модель трехслойного перцептрона и обучение этой модели методом градиентного спуска.\n",
    "\n",
    "На этот раз предлагается работать с реальными данными. Данные представляют из себя набор рукописных цифр. Это изображения размером 28х28. Каждому изображению поставлен в соответствие класс - арабская цифра. Задача модели - определить цифру, соответствующую произвольному изображению из тестового набора данных.\n",
    "\n",
    "Так же, как и в ДЗ №1, предлагается реализовать функцию потерь и саму модель перцептрона в манере, схожей с построением модулей фреймворка pytorch.\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "- (текст) формулировка задачи\n",
    "- (текст) формулировка признакового описания объектов\n",
    "- (текст) формулировка функции ошибки\n",
    "- (текст) формулировка меры качества модели\n",
    "- (текст, код и диаграммы) исследование исходных данных: распределение признаков и другие действия, дающие понимание о характере исходных данных\n",
    "- (текст, код, диаграммы) фильтрация признаков (при необходимости), порождение признаков (при необходимости)\n",
    "- (код, результаты, коммментарии) обучение модели методом градиентного спуска\n",
    "- (код, результаты, комментарии) оценка качества модели на валидационной выборке\n",
    "\n",
    "#### Код решения:\n",
    "(можно использовать предлагаемые шаблоны)\n",
    "- формулировка модели трехслойного перцептрона (имеется в виду только один скрытый слой);\n",
    "- формулировка функции ошибки;\n",
    "- формулировка метрики (метрик);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределение признаков;\n",
    "- распределение целевой переменной;\n",
    "- отдельные экземпляры выборки в виде изображений;\n",
    "- эволюция функции ошибки по ходу обучения;\n",
    "- эволюция метрики(метрик) по ходу обучения\n",
    "\n",
    "#### Выводы\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечание:<br />\n",
    "Реализация перцептрона и других составляющих исследования может быть написана только с использованием библиотеки Numpy или scipy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbGQU2nxyMD1"
   },
   "source": [
    "### Исходные данные\n",
    "\n",
    "Исходные данные можно скачать [по этой ссылке](https://www.dropbox.com/s/y6ar7i7mb6fvoed/mnist.npz). Набор данных MNIST поставляется в различных вариантах. В варианте, доступном по приведенной ссылке, чтение исходных данных может быть выполнено следующим образом:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjL1egcfyMD2"
   },
   "source": [
    "### Многослойный перцептрон\n",
    "\n",
    "Напомним, мы задаем мнолослойный перцептрон как сложную функцию, в которой используются линейные операции и поточечные нелинейные преобразования. Если входные данные (признаковое описание объекта) заданы вектором $x$, то функция перцептрона с одним скрытым слоем выглядит следующим образом:\n",
    "$$\n",
    "F(x) = \\Psi\\left(\\phi\\left( {x}\\cdot\\theta_1 + b_1 \\right)\\cdot\\theta_2 + b_2\\right),\n",
    "$$\n",
    "где $x$ имеется в виду без дополнительного единичного признака; $\\phi$ - функция активации скрытого слоя; $\\Psi$ - функция активации выходного слоя перцептрона.\n",
    "\n",
    "Напомним также, что в задаче жесткой многоклассовой классификации на $K$ классов допустим вариант формулировки модели, такой что:\n",
    "- количество признаков целевой переменной совпадает с количеством классов $K$;\n",
    "- в качестве функции активации $\\Psi$ может использоваться `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "- в качестве функции потерь может использоваться перектрестная энтропия в многоклассовом варианте (приведено в записи для одного объекта):\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = \\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$\n",
    "\n",
    "В своем решении вы никак не ограничены в выборе функций активации $\\phi$ или $\\Psi$. Однако есть некоторые устоявшиеся практики применения функций `ReLU, sigmoid, tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crj5ucVAyMD2"
   },
   "source": [
    "### Особенности реализации функции `softmax`\n",
    "Несложно заметить, что как в числителе, так и в знаменателе функции `softmax` стоит экспонента некоторого числа. При этом следует понимать, что разрядность чисел с плавающей точкой `float32`, `float64` и даже `float128` не бесконечны. Свойства экспоненты таковы, что, например, для переполнения разрядности чисел `float64` (максимум  $\\sim1.78*10^{308}$) достаточно показателя, превышающего 710, что совсем немного. Поэтому в случае практической реализации функции `softmax` имеет смысл предусмотреть случаи, когда аргументы экспоненты велики или, наоборот, слишком малы.\n",
    "\n",
    "В этом ДЗ кроме прочих заданий вам нужно реализовать вычислительно стабильную версию `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLxcy7C2yMD3"
   },
   "source": [
    "### Особенности вычисления градиента функции потерь\n",
    "\n",
    "В этом домашнем задании, также как и в ДЗ№1 предлагается реализовывать функцию потерь и отдельные вычислительные блоки перцептрона наследующими `Differentiable` для общности восприятия этих модулей как дифференцируемых по своим аргументам. По желанию можно вычислить градиент функции потерь по параметрам модели вручную (и далее реализовать его в коде), однако предпочитаемым способом будет реализация градиента каждого из вычислительных блоков по аргументу в методе `backward()` и использование этого результата в обобщенном виде, без упрощения. Этот вариант вычисления градиента функции потерь по параметрам модели называется \"backpropagation\" (\"метод обратного распространения ошибки\" или \"метод обратной волны\" у разных авторов).\n",
    "\n",
    "Нелишним будет напомнить, что в некоторых случаях для вычисления компоненты градиента необходимо хранить значения, полученные на этапе вычисления функции $F(x)$. В вашем решении это может быть устроено по-разному. Но для тех, кто хочет придерживаться предложенного шаблона, введен атрибут `state` класса `Differentiable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybaUJTYyyMD4"
   },
   "source": [
    "# Моё решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aDN0ZU4nyMD4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dxHg0x2ayMD9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nuCqdZTHyMD_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4V2Tpzp3yMEC"
   },
   "outputs": [],
   "source": [
    "mnist = np.load('./mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "# Обратите внимание на то, что целевая переменная в виде целых чисел от 0 до 9, в то время как в формулах,\n",
    "# приведенных выше, подразумевается one-hot кодирование целевой переменной\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yto9jMzJyMEF",
    "outputId": "a426d5b4-392f-49b7-f165-6018ca7f9fdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qa6FPS9yMEI",
    "outputId": "85645b94-2b83-48cf-cc49-b7774ef9efb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZLKKbeUzyMEL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "dbGNXZIxyMEO",
    "outputId": "54868f75-1cdc-4534-dc96-2108b5a2ce51"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_train[0, :, :]) # пример данных\n",
    "plt.show()\n",
    "print(y_train[0]) # такргетное значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FI7zsiDuyMEQ"
   },
   "outputs": [],
   "source": [
    "X_train = x_train.reshape([-1, 28 * 28]) # переводим трёхмерный тензор в двумерный\n",
    "X_test = x_test.reshape([-1, 28 * 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjX3LGVFyMET",
    "outputId": "f56d4dfa-41cd-4903-b635-7a0f9f50aecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "VDDhkDqUyMEW"
   },
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po4u6MtbyMEY"
   },
   "source": [
    "в качестве функции потерь используем перектрестную энтропию в многоклассовом варианте:\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = \\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$, то есть вероятность принадлежать классу, которую предсказывает наша модель. В векторном виде $\\vec{y^T}  \\ln \\vec{\\hat{y}}$. Градиент по входному значению $ \\vec{\\hat{y}} $ равен $ \\frac{\\vec{y}}{\\hat{y}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "2RXUe0YHyMEZ"
   },
   "outputs": [],
   "source": [
    "class loss(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(y_true, y_pred)\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        loss_value = 0.0\n",
    "        self.state = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        y_predT = np.log(y_pred).T\n",
    "        loss_value = sum(y_true.dot(y_predT)[0])\n",
    "                \n",
    "        return -loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу y_pred\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента y_pred\n",
    "        \n",
    "        partial_grad = np.zeros_like(y_pred)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #print(y_true.shape)\n",
    "        #print(y_true)\n",
    "        \n",
    "        partial_grad = -y_true / y_pred\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okidTMFWyMEb"
   },
   "source": [
    "Этот класс мы прописали в предыдущем задании. Только теперь в backward реализуем вычисление градиента абстрактно от входного вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "eBXKlVRPyMEc"
   },
   "outputs": [],
   "source": [
    "class Linear(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = y_in\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert X.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "            self.theta = np.random.normal(scale=0.01, size = (X.shape[1], 100))\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        y_pred = X.dot(self.theta)\n",
    "        self.state = y_pred # сохраняем результат вычисления функции на данном этапе\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "4jxelHiEyMEe"
   },
   "outputs": [],
   "source": [
    "class Linear_2(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear_2, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in, flag):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "\n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        if (flag == 1): # градиент по параметрам \n",
    "            partial_grad = y_in\n",
    "        else:\n",
    "            partial_grad = self.theta\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert X.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "            self.theta = np.random.normal(scale=0.01, size = (100, 10))\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        y_pred =  X.dot(self.theta)\n",
    "        self.state = y_pred # сохраняем результат вычисления функции на данном этапе\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAf690MGyMEh"
   },
   "source": [
    "Возьмём функцию активации `sigmoid`\n",
    "\n",
    "$$\\begin{equation}sigmoid=\\frac{1}{1+e^{-x}}\\end{equation}$$\n",
    "Градиент этой функции находится следующим образом:\n",
    "$$ \\begin{equation}\\frac{d (sigmoid)}{d x}=-\\frac{1}{\\left(1+e^{-x}\\right)^{2}}\\left(-e^{-x}\\right)=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}}\\end{equation} $$\n",
    "\n",
    "$$\\begin{equation}=\\frac{1}{1+e^{-x}}\\left(1-\\frac{1}{1+e^{-x}}\\right)=sigmoid(x)(1-sigmoid(x))\\end{equation} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "j74iughpyMEh"
   },
   "outputs": [],
   "source": [
    "class Activation(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Activation, self).__init__()\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        self.activ = Activation()\n",
    "        partial_grad = self.activ.forward(y_in) * (1 - self.activ.forward(y_in))\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, y_in):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        \n",
    "        y_pred = 1.0/(1.0 + np.exp(-y_in))\n",
    "        self.state = y_pred # сохраняем результат вычисления функции на данном этапе\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKOgUxtPyMEk"
   },
   "source": [
    "в качестве функции активации $\\Psi$ используем `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "Как описано в условии задания `softmax` не численно стабильный и нужно рассмотреть случаи, когда показатель экспоненты либо очень большое число, или наоборот достаточно маленькое.\n",
    "\n",
    "Для защиты от этого воспользуемся свойством `softmax`, что $softmax(x) = softmax(x+c)$ для любой константы $c$. Следовательно, можем вычисть максимальный элемент в исходном векторе из всех элементов, то есть для входного вектора $x$ определим $z$ так, чтобы: $z = x - \\max(x) $ и затем будем брать `softmax` от вектора z, который будет стабильным "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ7j5d0qyMEk"
   },
   "source": [
    "Вычисляем градиент от `softmax`: \n",
    "\n",
    "$$\\begin{equation}\\frac{\\partial \\operatorname{smax}\\left(x_{i}\\right)}{\\partial x_{i}}=\\frac{f^{\\prime}(x) g(x)-f(x) g^{\\prime}(x)}{g(x)^{2}}\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation}=\\frac{e^{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}-e^{x_{i}} \\frac{\\partial}{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}}{\\left(\\sum_{j=1}^{|X|} e^{x_{j}}\\right)^{2}}\\end{equation} $$\n",
    "\n",
    "$$\\begin{equation}=\\frac{e^{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}-\\left(e^{x_{i}}\\right)^{2}}{\\left(\\sum_{j=1}^{|X|} e^{x_{j}}\\right)^{2}}\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation}=\\left(\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}} \\frac{\\sum_{j=1}^{|X|} e^{x_{j}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)-\\left(\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}} \\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)\\end{equation}$$\n",
    "\n",
    "$$ \\begin{equation}=\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\left(1-\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}=\\operatorname{smax}\\left(x_{i}\\right)\\left(1-\\operatorname{smax}\\left(x_{i}\\right)\\right)\\end{equation} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHAd3PEAyMEl"
   },
   "source": [
    "В numpy: $smax(\\vec{x}) \\times (1 - smax(\\vec{x}))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "Z48HFbOwyMEl"
   },
   "outputs": [],
   "source": [
    "class Softmax(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        self.sm = Softmax()\n",
    "        partial_grad = self.sm.forward(y_in) * (1 - self.sm.forward(y_in))\n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, y_in):  \n",
    "        y_pred = np.zeros_like(y_in)\n",
    "        i = 0\n",
    "        for string in y_in:\n",
    "            z = string - np.max(string) #для того чтобы сделать softmax численно стабильным\n",
    "            denominator = 0.0\n",
    "            for el in z:\n",
    "                denominator += np.exp(el)\n",
    "\n",
    "            y_pred[i] = [np.exp(a) / denominator for a in z]\n",
    "            i += 1\n",
    "        self.state = y_pred # сохраняем результат вычисления функции на данном этапе\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "4I4YZn1yyMEn"
   },
   "outputs": [],
   "source": [
    "class Perceptron(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.lin_1 = Linear()\n",
    "        self.lin_2 = Linear_2()\n",
    "        self.psi   = Softmax()\n",
    "        self.fi    = Activation()\n",
    "        self.modules = [self.lin_1, self.fi, self.lin_2, self.psi] # список в каком порядке вычисляется функция\n",
    "        self.parameters = [self.lin_1.theta, self.lin_2.theta] # список параметров\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, X, loss_backward):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        #grad = [0.0, 0.0]\n",
    "        \n",
    "        \n",
    "        grad_theta2 = self.lin_2.backward(self.fi.state,1).T.dot(self.psi.backward(self.lin_2.state) * loss_backward)\n",
    "        #print(grad[0].shape)\n",
    "        grad_fitrst_part = (loss_backward * self.psi.backward(self.lin_2.state)).dot(self.lin_2.backward(self.fi.state,0).T) * self.fi.backward(self.lin_1.state)\n",
    "        grad_second_part = self.lin_1.backward(X).T.dot(grad_fitrst_part) \n",
    "        grad_theta1 = grad_second_part\n",
    "        #print(grad[1].shape)\n",
    "        \n",
    "        #partial_grad = grad\n",
    "        self.parameters = [self.lin_1.theta, self.lin_2.theta]\n",
    "        \n",
    "        return grad_theta1, grad_theta2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = X\n",
    "\n",
    "        if (self.lin_2.theta is not None):\n",
    "            self.lin_1.theta = self.parameters[0]\n",
    "            self.lin_2.theta = self.parameters[1]\n",
    "    \n",
    "        for el in self.modules:\n",
    "            #print(y_pred.shape)\n",
    "            y_pred = el.forward(y_pred)\n",
    "\n",
    "        self.parameters[0] = self.lin_1.theta\n",
    "        self.parameters[1] = self.lin_2.theta\n",
    "        #print(self.lin_2.theta)\n",
    "        \n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2A3KU73yMEr"
   },
   "source": [
    "Задаём шаг градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "u_QgjvjdyMEr"
   },
   "outputs": [],
   "source": [
    "lr = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "bes8CBNXyMEt"
   },
   "outputs": [],
   "source": [
    "batch_size = 5000 # выбираем размер батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные все не влезают в память и выходит ошибка memory error, поэтому бьём на батчи, после каждого прохода по данным, то есть после каждой эпохи. Подробные комментарии есть в самом коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "hbPXqy87yMEu"
   },
   "outputs": [],
   "source": [
    "def train_loop(X, y, model, loss_fn, x_test, y_test, epochs=10):\n",
    "    loss_history = []\n",
    "    loss_history_test = []\n",
    "    pbar = tqdm(total=epochs)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        order = np.random.permutation(len(X)) # каждую эпоху перемешиваем\n",
    "                # порядок примеров\n",
    "        batch_loss = []\n",
    "        for start_index in range(0, len(X), batch_size): # градиентый шаг делаем для train данных размера batch\n",
    "            batch_indexes = order[start_index:start_index+batch_size]\n",
    "            \n",
    "            X_batch = X[batch_indexes]\n",
    "            y_batch = y[batch_indexes]\n",
    "\n",
    "            y_pred = model(X_batch) # получили y_pred, что предсказала наша модель\n",
    "            loss_value = loss_fn(y_batch, y_pred)\n",
    "            print(loss_value)\n",
    "            loss_backward = loss_fn.backward(y_batch,y_pred) # подсчитали градиент в лоссе\n",
    "            #print(loss_backward.shape)\n",
    "            grad_theta1, grad_theta2 = model.backward(X_batch, loss_backward)\n",
    "\n",
    "            model.parameters[0] = model.parameters[0] - lr * grad_theta1 # сделали градиентный шаг\n",
    "            model.parameters[1] = model.parameters[1] - lr * grad_theta2\n",
    "            \n",
    "            batch_loss.append(loss_value)\n",
    "        y_pred_test = model(x_test)\n",
    "        test_loss = loss_fn(y_test, y_pred_test) # считаем лосс на тесте после каждой эпохи\n",
    "        loss_history_test.append(test_loss)\n",
    "        loss_history.append(np.mean(batch_loss)) # усредняем лосс по батчам, чтобы получить лосс за 1 эпоху\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'loss': np.mean(batch_loss)})\n",
    "    pbar.close()\n",
    "    return loss_history, loss_history_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUHdmF9ehF33"
   },
   "source": [
    "Добавляем баес для тестовой матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "GGlaUwh0yMEw"
   },
   "outputs": [],
   "source": [
    "bias = np.ones((X_train.shape[0], 1))\n",
    "Xtr = np.hstack((X_train, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5BHSPtPyMEy"
   },
   "source": [
    "Для того, чтобы использовать софтмакс на выходе делаем onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "tCkGp_DKVfiT"
   },
   "outputs": [],
   "source": [
    "def To_one_hot_encoding(y):\n",
    "    y_one_hot = np.zeros((y.shape[0], 10))\n",
    "    for i in range(len(y_one_hot)):\n",
    "        y_one_hot[i][y[i]] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZQqO8qIWDCQ",
    "outputId": "6be47268-0943-436c-c613-9ca2c3575402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train_one_hot = To_one_hot_encoding(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_test_one_hot = To_one_hot_encoding(y_test)\n",
    "print(y_train_one_hot[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yzrs_95bg0qQ",
    "outputId": "66a9b2ea-4994-4270-9b59-1283f9440cd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = np.ones((X_test.shape[0], 1))\n",
    "Xtest = np.hstack((X_test, bias))\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBvkmz7QyME2",
    "outputId": "b687d16b-172c-4a71-cd71-30dd36d2b57d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11619.00957534076\n",
      "11663.048563531844\n",
      "11048.635988119564\n",
      "11758.938213053958\n",
      "11892.506959251508\n",
      "10947.199947364163\n",
      "11165.56704081825\n",
      "11583.211064889872\n",
      "10352.84407750587\n",
      "11201.083398267067\n",
      "9847.594913876357\n",
      "11705.945933119943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 1/20 [00:24<07:45, 24.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 1/20 [00:24<07:45, 24.47s/it, loss=1.12e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11977.968011177476\n",
      "11855.4149149035\n",
      "12129.938786467286\n",
      "11138.938483849652\n",
      "11020.804018910801\n",
      "12473.083740361762\n",
      "12361.847128824656\n",
      "11646.317446527102\n",
      "10987.122769714908\n",
      "11705.953105276589\n",
      "11876.18682490475\n",
      "8330.034656280643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 2/20 [00:47<07:10, 23.90s/it, loss=1.12e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 2/20 [00:47<07:10, 23.90s/it, loss=1.15e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11876.175234646977\n",
      "11745.569980515907\n",
      "12005.496523846292\n",
      "10674.851331052345\n",
      "12373.41471661789\n",
      "12297.43156746017\n",
      "12543.98612499598\n",
      "12199.085376674397\n",
      "10990.154967368506\n",
      "10481.424867336782\n",
      "15867.086186058736\n",
      "12343.233034080884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 3/20 [01:09<06:36, 23.34s/it, loss=1.15e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 3/20 [01:09<06:36, 23.34s/it, loss=1.21e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12402.640847174178\n",
      "12593.655228583673\n",
      "10818.456507670575\n",
      "12070.620060797857\n",
      "12164.710794824123\n",
      "12557.164909630555\n",
      "12613.36288893578\n",
      "12444.937402365766\n",
      "10817.233389325966\n",
      "8003.34253288545\n",
      "17035.388362716436\n",
      "12049.99837342205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 4/20 [01:30<06:04, 22.77s/it, loss=1.21e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 4/20 [01:30<06:04, 22.77s/it, loss=1.21e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7983.909853051819\n",
      "12276.183125134661\n",
      "12741.464616404106\n",
      "12385.14623246953\n",
      "8020.401232240738\n",
      "10691.497942057807\n",
      "10074.272212914935\n",
      "12863.253615702191\n",
      "11662.806199550327\n",
      "12912.612429526638\n",
      "12572.46140721228\n",
      "11936.718426721442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 5/20 [01:52<05:39, 22.62s/it, loss=1.21e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 5/20 [01:52<05:39, 22.62s/it, loss=1.13e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11896.051391724966\n",
      "10082.881556975191\n",
      "8010.707764635213\n",
      "10670.766336355378\n",
      "17958.490908916854\n",
      "8031.333530224453\n",
      "11854.504616154543\n",
      "12063.411792738898\n",
      "12841.799202851633\n",
      "11889.181995727275\n",
      "10051.880817318406\n",
      "12613.052191146931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 6/20 [02:14<05:14, 22.43s/it, loss=1.13e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 6/20 [02:14<05:14, 22.43s/it, loss=1.15e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12114.433758407968\n",
      "10755.891418537822\n",
      "18289.232635524168\n",
      "10006.991767863925\n",
      "12443.107353286945\n",
      "12300.935131819564\n",
      "11886.665113889538\n",
      "12145.599161392262\n",
      "11981.244232924373\n",
      "9790.40688078712\n",
      "9832.323687460712\n",
      "11868.798589493588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 7/20 [02:36<04:50, 22.37s/it, loss=1.15e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 7/20 [02:37<04:50, 22.37s/it, loss=1.2e+4] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11767.456685223115\n",
      "12885.941228088846\n",
      "10659.568006203517\n",
      "10617.552058935453\n",
      "12637.513066070127\n",
      "11850.672601869604\n",
      "12502.048342959308\n",
      "9994.238481583936\n",
      "12353.450214066339\n",
      "19335.05819266127\n",
      "12534.544177186566\n",
      "10547.856752526497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 8/20 [02:59<04:30, 22.54s/it, loss=1.2e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 8/20 [02:59<04:30, 22.54s/it, loss=1.23e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8037.6462101537245\n",
      "19261.465191788455\n",
      "12902.122528501957\n",
      "11494.166126322734\n",
      "12057.215529765535\n",
      "9866.62874224195\n",
      "8053.251498529784\n",
      "9983.096484595248\n",
      "11448.772001319676\n",
      "9841.758190879364\n",
      "8247.517587446886\n",
      "12940.450717457681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 9/20 [03:22<04:07, 22.50s/it, loss=1.23e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 9/20 [03:22<04:07, 22.50s/it, loss=1.12e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10558.917814986944\n",
      "11869.28262400577\n",
      "12921.40356159963\n",
      "12879.129399564239\n",
      "11906.86938494529\n",
      "9942.906046497921\n",
      "19816.913433246995\n",
      "7939.451759012535\n",
      "12679.497700463273\n",
      "12106.403753557419\n",
      "12055.264548997602\n",
      "9757.748997249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 10/20 [03:44<03:44, 22.47s/it, loss=1.12e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 10/20 [03:44<03:44, 22.47s/it, loss=1.2e+4] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8091.963904169582\n",
      "12150.7721757686\n",
      "9874.306911110201\n",
      "11802.776664751651\n",
      "11624.117556368323\n",
      "12999.722559250993\n",
      "12144.640201359063\n",
      "11745.434655385152\n",
      "7968.8435719262\n",
      "11652.053616507277\n",
      "9934.985603631556\n",
      "11698.212410021042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11/20 [04:07<03:22, 22.52s/it, loss=1.2e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11/20 [04:07<03:22, 22.52s/it, loss=1.1e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10460.616525258869\n",
      "9813.143496065075\n",
      "12849.61299298546\n",
      "8315.59543722032\n",
      "8234.302190675486\n",
      "8057.265288488182\n",
      "20656.2901385295\n",
      "7971.029226175861\n",
      "12906.623656031039\n",
      "11679.823536127993\n",
      "20631.28492527088\n",
      "8038.842937403803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 12/20 [04:29<02:58, 22.26s/it, loss=1.1e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 12/20 [04:29<02:58, 22.26s/it, loss=1.16e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12708.33451292299\n",
      "8163.402642680457\n",
      "20786.419990714287\n",
      "11683.983036756848\n",
      "12016.646969446007\n",
      "11728.64482925199\n",
      "12155.174172437168\n",
      "12075.396827219154\n",
      "10710.363656168392\n",
      "10656.188748900335\n",
      "12902.737063759858\n",
      "10407.740518092378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13/20 [04:50<02:34, 22.08s/it, loss=1.16e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13/20 [04:50<02:34, 22.08s/it, loss=1.22e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11810.995145515939\n",
      "11792.005907684133\n",
      "12030.724506189013\n",
      "21028.915071030646\n",
      "12342.17332297262\n",
      "12345.16610594599\n",
      "10540.585824100306\n",
      "12222.761254455192\n",
      "10415.625006559818\n",
      "12704.797314360323\n",
      "12003.40707138582\n",
      "12075.001066436267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 14/20 [05:13<02:14, 22.34s/it, loss=1.22e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 14/20 [05:13<02:14, 22.34s/it, loss=1.26e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21165.407789698864\n",
      "12018.90925062311\n",
      "11643.833846647272\n",
      "11653.937881577913\n",
      "12690.974030736788\n",
      "8172.460800404738\n",
      "12253.045561426443\n",
      "12164.409004288464\n",
      "11906.360222941807\n",
      "21078.77279866326\n",
      "11694.585921763222\n",
      "9892.374466516538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15/20 [05:36<01:52, 22.48s/it, loss=1.26e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15/20 [05:36<01:52, 22.48s/it, loss=1.3e+4] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10456.384789031905\n",
      "21499.468275464777\n",
      "21509.2650976431\n",
      "12725.616502829344\n",
      "11790.319322294903\n",
      "11950.180844445966\n",
      "12910.354711691405\n",
      "12964.136246622245\n",
      "12771.767892571212\n",
      "11990.640038774936\n",
      "12848.462775721484\n",
      "11737.221323809903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 16/20 [05:58<01:28, 22.24s/it, loss=1.3e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 16/20 [05:58<01:28, 22.24s/it, loss=1.38e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12079.859748766861\n",
      "11831.862137970893\n",
      "8003.667826478885\n",
      "22165.66299875396\n",
      "12847.485671363751\n",
      "11613.41359388527\n",
      "11706.166693315316\n",
      "10702.025898554799\n",
      "12056.98032606061\n",
      "12012.638605910468\n",
      "12820.268037007061\n",
      "11679.015480308937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17/20 [06:20<01:06, 22.20s/it, loss=1.38e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17/20 [06:20<01:06, 22.20s/it, loss=1.25e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504.283497016826\n",
      "12137.447350380593\n",
      "11664.367384507244\n",
      "10489.583698406126\n",
      "12282.43745578518\n",
      "11827.325871247436\n",
      "9609.755039229936\n",
      "12113.94055292609\n",
      "12042.47911732891\n",
      "8105.419607685263\n",
      "8026.17785594322\n",
      "12002.711353476956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 18/20 [06:42<00:44, 22.13s/it, loss=1.25e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 18/20 [06:42<00:44, 22.13s/it, loss=1.09e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12078.966037902634\n",
      "12768.938077138015\n",
      "8195.191030712076\n",
      "22049.200203181976\n",
      "21912.512818570944\n",
      "10438.7229680398\n",
      "9624.322874435811\n",
      "11716.74423433806\n",
      "12104.411444901629\n",
      "13008.523621586215\n",
      "21964.1721223795\n",
      "11630.989169375749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19/20 [07:04<00:22, 22.25s/it, loss=1.09e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19/20 [07:04<00:22, 22.25s/it, loss=1.4e+4] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10584.91623993599\n",
      "12687.876208640384\n",
      "22209.02095148648\n",
      "12575.293438534809\n",
      "12186.474218104691\n",
      "7816.710390342948\n",
      "12100.950394394444\n",
      "11805.781975909023\n",
      "12785.82275875719\n",
      "7940.2523542206045\n",
      "11833.360470060712\n",
      "12725.978221326597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 20/20 [07:26<00:00, 22.17s/it, loss=1.4e+4]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 20/20 [07:26<00:00, 22.34s/it, loss=1.23e+4]\n"
     ]
    }
   ],
   "source": [
    "obj_fn = loss()\n",
    "model = Perceptron()\n",
    "train_loss_history, test_loss_history = train_loop(Xtr, y_train_one_hot, model, obj_fn, Xtest, y_test_one_hot, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "построим график, как меняется лосс на трейне и на валидации от эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "L_NHJnslyME4",
    "outputId": "25871254-c2e0-40dc-cf00-f4b5f845c8a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$loss$')"
      ]
     },
     "execution_count": 182,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history, label='loss function train') # график убывания loss функции от номера эпохи\n",
    "plt.plot(test_loss_history, label='loss function test', c='r')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('$n\\_epoxa$')\n",
    "plt.ylabel('$loss$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не сходится(("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbch8X1xyME6"
   },
   "source": [
    "# Мои выводы\n",
    "1) Всё, что написали, как видим из графика выше не заработало и loss функция на трейне не падает((( (где ошибка я не знаю, мб неправильно считаю градиент, но вроде бы всё перепроверяла) \n",
    "\n",
    "2) столкнулась с проблемой, что данные не влезали в память для потчёта и пришлось разбивать на batch\n",
    "\n",
    "3) наверное, нужно использовать регуляризацию и проблема в том, что не сходится состоит именно в этом, но не успела попробовать, да и нет идей, как тут это делать\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
