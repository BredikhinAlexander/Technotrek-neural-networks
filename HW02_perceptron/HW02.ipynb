{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №2 - обучение модели трехслойного перцептрона методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)\n",
    "В качестве теоретического задания в этом ДЗ предлагается провести вывод функции ошибки для задачи регрессии в предположении, что целевая переменная подчиняется распределению Лапласа. Также предлагается воспользоваться байесовским выводом и в том же предположении относительно распределения целевой переменной вывести форму функции потерь с условием лапласовского априорного распределения параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)\n",
    "В этом ДЗ предлагается реализовать модель трехслойного перцептрона и обучение этой модели методом градиентного спуска.\n",
    "\n",
    "На этот раз предлагается работать с реальными данными. Данные представляют из себя набор рукописных цифр. Это изображения размером 28х28. Каждому изображению поставлен в соответствие класс - арабская цифра. Задача модели - определить цифру, соответствующую произвольному изображению из тестового набора данных.\n",
    "\n",
    "Так же, как и в ДЗ №1, предлагается реализовать функцию потерь и саму модель перцептрона в манере, схожей с построением модулей фреймворка pytorch.\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "- (текст) формулировка задачи\n",
    "- (текст) формулировка признакового описания объектов\n",
    "- (текст) формулировка функции ошибки\n",
    "- (текст) формулировка меры качества модели\n",
    "- (текст, код и диаграммы) исследование исходных данных: распределение признаков и другие действия, дающие понимание о характере исходных данных\n",
    "- (текст, код, диаграммы) фильтрация признаков (при необходимости), порождение признаков (при необходимости)\n",
    "- (код, результаты, коммментарии) обучение модели методом градиентного спуска\n",
    "- (код, результаты, комментарии) оценка качества модели на валидационной выборке\n",
    "\n",
    "#### Код решения:\n",
    "(можно использовать предлагаемые шаблоны)\n",
    "- формулировка модели трехслойного перцептрона (имеется в виду только один скрытый слой);\n",
    "- формулировка функции ошибки;\n",
    "- формулировка метрики (метрик);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределение признаков;\n",
    "- распределение целевой переменной;\n",
    "- отдельные экземпляры выборки в виде изображений;\n",
    "- эволюция функции ошибки по ходу обучения;\n",
    "- эволюция метрики(метрик) по ходу обучения\n",
    "\n",
    "#### Выводы\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечание:<br />\n",
    "Реализация перцептрона и других составляющих исследования может быть написана только с использованием библиотеки Numpy или scipy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные данные\n",
    "\n",
    "Исходные данные можно скачать [по этой ссылке](https://www.dropbox.com/s/y6ar7i7mb6fvoed/mnist.npz). Набор данных MNIST поставляется в различных вариантах. В варианте, доступном по приведенной ссылке, чтение исходных данных может быть выполнено следующим образом:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойный перцептрон\n",
    "\n",
    "Напомним, мы задаем мнолослойный перцептрон как сложную функцию, в которой используются линейные операции и поточечные нелинейные преобразования. Если входные данные (признаковое описание объекта) заданы вектором $x$, то функция перцептрона с одним скрытым слоем выглядит следующим образом:\n",
    "$$\n",
    "F(x) = \\Psi\\left(\\phi\\left( {x}\\cdot\\theta_1 + b_1 \\right)\\cdot\\theta_2 + b_2\\right),\n",
    "$$\n",
    "где $x$ имеется в виду без дополнительного единичного признака; $\\phi$ - функция активации скрытого слоя; $\\Psi$ - функция активации выходного слоя перцептрона.\n",
    "\n",
    "Напомним также, что в задаче жесткой многоклассовой классификации на $K$ классов допустим вариант формулировки модели, такой что:\n",
    "- количество признаков целевой переменной совпадает с количеством классов $K$;\n",
    "- в качестве функции активации $\\Psi$ может использоваться `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "- в качестве функции потерь может использоваться перектрестная энтропия в многоклассовом варианте (приведено в записи для одного объекта):\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = \\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$\n",
    "\n",
    "В своем решении вы никак не ограничены в выборе функций активации $\\phi$ или $\\Psi$. Однако есть некоторые устоявшиеся практики применения функций `ReLU, sigmoid, tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности реализации функции `softmax`\n",
    "Несложно заметить, что как в числителе, так и в знаменателе функции `softmax` стоит экспонента некоторого числа. При этом следует понимать, что разрядность чисел с плавающей точкой `float32`, `float64` и даже `float128` не бесконечны. Свойства экспоненты таковы, что, например, для переполнения разрядности чисел `float64` (максимум  $\\sim1.78*10^{308}$) достаточно показателя, превышающего 710, что совсем немного. Поэтому в случае практической реализации функции `softmax` имеет смысл предусмотреть случаи, когда аргументы экспоненты велики или, наоборот, слишком малы.\n",
    "\n",
    "В этом ДЗ кроме прочих заданий вам нужно реализовать вычислительно стабильную версию `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности вычисления градиента функции потерь\n",
    "\n",
    "В этом домашнем задании, также как и в ДЗ№1 предлагается реализовывать функцию потерь и отдельные вычислительные блоки перцептрона наследующими `Differentiable` для общности восприятия этих модулей как дифференцируемых по своим аргументам. По желанию можно вычислить градиент функции потерь по параметрам модели вручную (и далее реализовать его в коде), однако предпочитаемым способом будет реализация градиента каждого из вычислительных блоков по аргументу в методе `backward()` и использование этого результата в обобщенном виде, без упрощения. Этот вариант вычисления градиента функции потерь по параметрам модели называется \"backpropagation\" (\"метод обратного распространения ошибки\" или \"метод обратной волны\" у разных авторов).\n",
    "\n",
    "Нелишним будет напомнить, что в некоторых случаях для вычисления компоненты градиента необходимо хранить значения, полученные на этапе вычисления функции $F(x)$. В вашем решении это может быть устроено по-разному. Но для тех, кто хочет придерживаться предложенного шаблона, введен атрибут `state` класса `Differentiable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Моё решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load('./mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "# Обратите внимание на то, что целевая переменная в виде целых чисел от 0 до 9, в то время как в формулах,\n",
    "# приведенных выше, подразумевается one-hot кодирование целевой переменной\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOYElEQVR4nO3dbYxc5XnG8euKbUwxJvHGseMQFxzjFAg0Jl0ZkBFQoVCCIgGKCLGiiFBapwlOQutKUFoVWtHKrRIiSimSKS6m4iWQgPAHmsSyECRqcFmoAROHN+MS4+0aswIDIfZ6fffDjqsFdp5dZs68eO//T1rNzLnnzLk1cPmcmeeceRwRAjD5faDTDQBoD8IOJEHYgSQIO5AEYQeSmNrOjR3i6XGoZrRzk0Aqv9Fb2ht7PFatqbDbPkfS9ZKmSPrXiFhVev6hmqGTfVYzmwRQsDE21K01fBhve4qkGyV9TtLxkpbZPr7R1wPQWs18Zl8i6fmI2BoReyXdJem8atoCULVmwn6kpF+Nery9tuwdbC+33We7b0h7mtgcgGY0E/axvgR4z7m3EbE6InojoneapjexOQDNaCbs2yXNH/X445J2NNcOgFZpJuyPSlpke4HtQyR9SdK6atoCULWGh94iYp/tFZJ+rJGhtzUR8XRlnQGoVFPj7BHxgKQHKuoFQAtxuiyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJNDWLK7qfp5b/E0/5yOyWbv+ZPz+6bm34sP3FdY9auLNYP+wbLtb/97pD6tYe7/1+cd1dw28V6yffs7JYP+bPHinWO6GpsNveJukNScOS9kVEbxVNAaheFXv234+IXRW8DoAW4jM7kESzYQ9JP7H9mO3lYz3B9nLbfbb7hrSnyc0BaFSzh/FLI2KH7TmS1tv+ZUQ8PPoJEbFa0mpJOsI90eT2ADSoqT17ROyo3e6UdJ+kJVU0BaB6DYfd9gzbMw/cl3S2pM1VNQagWs0cxs+VdJ/tA69zR0T8qJKuJpkpxy0q1mP6tGJ9xxkfKtbfPqX+mHDPB8vjxT/9dHm8uZP+49czi/V/+OdzivWNJ95Rt/bi0NvFdVcNfLZY/9hPD75PpA2HPSK2Svp0hb0AaCGG3oAkCDuQBGEHkiDsQBKEHUiCS1wrMHzmZ4r16269sVj/5LT6l2JOZkMxXKz/9Q1fLdanvlUe/jr1nhV1azNf3ldcd/qu8tDcYX0bi/VuxJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0C05/ZUaw/9pv5xfonpw1U2U6lVvafUqxvfbP8U9S3LvxB3drr+8vj5HP/6T+L9VY6+C5gHR97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHtG1E8wj1xss9q2/a6xeAlpxbru88p/9zzlCcPL9af+MYN77unA67d9bvF+qNnlMfRh197vViPU+v/APG2bxVX1YJlT5SfgPfYGBu0OwbHnMuaPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4exeYMvvDxfrwq4PF+ot31B8rf/r0NcV1l/z9N4v1OTd27ppyvH9NjbPbXmN7p+3No5b12F5v+7na7awqGwZQvYkcxt8q6d2z3l8paUNELJK0ofYYQBcbN+wR8bCkdx9Hnidpbe3+WknnV9wXgIo1+gXd3Ijol6Ta7Zx6T7S93Haf7b4h7WlwcwCa1fJv4yNidUT0RkTvNE1v9eYA1NFo2Adsz5Ok2u3O6loC0AqNhn2dpItr9y+WdH817QBolXF/N972nZLOlDTb9nZJV0taJelu25dKeknSha1scrIb3vVqU+sP7W58fvdPffkXxforN00pv8D+8hzr6B7jhj0iltUpcXYMcBDhdFkgCcIOJEHYgSQIO5AEYQeSYMrmSeC4K56tW7vkxPKgyb8dtaFYP+PCy4r1md9/pFhH92DPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+CZSmTX7168cV131p3dvF+pXX3las/8UXLyjW478/WLc2/+9+XlxXbfyZ8wzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEkzZnNzgH55arN9+9XeK9QVTD21425+6bUWxvujm/mJ939ZtDW97smpqymYAkwNhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKYuniYv2IVduL9Ts/8eOGt33sg39UrP/O39S/jl+Shp/b2vC2D1ZNjbPbXmN7p+3No5ZdY/tl25tqf+dW2TCA6k3kMP5WSeeMsfx7EbG49vdAtW0BqNq4YY+IhyUNtqEXAC3UzBd0K2w/WTvMn1XvSbaX2+6z3TekPU1sDkAzGg37TZIWSlosqV/Sd+s9MSJWR0RvRPRO0/QGNwegWQ2FPSIGImI4IvZLulnSkmrbAlC1hsJue96ohxdI2lzvuQC6w7jj7LbvlHSmpNmSBiRdXXu8WFJI2ibpaxFRvvhYjLNPRlPmzinWd1x0TN3axiuuL677gXH2RV9+8exi/fXTXi3WJ6PSOPu4k0RExLIxFt/SdFcA2orTZYEkCDuQBGEHkiDsQBKEHUiCS1zRMXdvL0/ZfJgPKdZ/HXuL9c9/8/L6r33fxuK6Byt+ShoAYQeyIOxAEoQdSIKwA0kQdiAJwg4kMe5Vb8ht/2nln5J+4cLylM0nLN5WtzbeOPp4bhg8qVg/7P6+pl5/smHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+ybn3hGL92W+Vx7pvXrq2WD/90PI15c3YE0PF+iODC8ovsH/cXzdPhT07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOPtBYOqCo4r1Fy75WN3aNRfdVVz3C4fvaqinKlw10FusP3T9KcX6rLXl353HO427Z7c93/aDtrfYftr2t2vLe2yvt/1c7XZW69sF0KiJHMbvk7QyIo6TdIqky2wfL+lKSRsiYpGkDbXHALrUuGGPiP6IeLx2/w1JWyQdKek8SQfOpVwr6fxWNQmgee/rCzrbR0s6SdJGSXMjol8a+QdB0pw66yy33We7b0h7musWQMMmHHbbh0v6oaTLI2L3RNeLiNUR0RsRvdM0vZEeAVRgQmG3PU0jQb89Iu6tLR6wPa9WnydpZ2taBFCFcYfebFvSLZK2RMR1o0rrJF0saVXt9v6WdDgJTD36t4v1139vXrF+0d/+qFj/kw/dW6y30sr+8vDYz/+l/vBaz63/VVx31n6G1qo0kXH2pZK+Iukp25tqy67SSMjvtn2ppJckXdiaFgFUYdywR8TPJI05ubuks6ptB0CrcLoskARhB5Ig7EAShB1IgrADSXCJ6wRNnffRurXBNTOK6359wUPF+rKZAw31VIUVL59WrD9+U3nK5tk/2Fys97zBWHm3YM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkGWff+wflny3e+6eDxfpVxzxQt3b2b73VUE9VGRh+u27t9HUri+se+1e/LNZ7XiuPk+8vVtFN2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJpxtm3nV/+d+3ZE+9p2bZvfG1hsX79Q2cX6x6u9+O+I4699sW6tUUDG4vrDhermEzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEo6I8hPs+ZJuk/RRjVy+vDoirrd9jaQ/lvRK7alXRUT9i74lHeGeONlM/Aq0ysbYoN0xOOaJGRM5qWafpJUR8bjtmZIes72+VvteRHynqkYBtM5E5mfvl9Rfu/+G7S2Sjmx1YwCq9b4+s9s+WtJJkg6cg7nC9pO219ieVWed5bb7bPcNaU9TzQJo3ITDbvtwST+UdHlE7JZ0k6SFkhZrZM//3bHWi4jVEdEbEb3TNL2ClgE0YkJhtz1NI0G/PSLulaSIGIiI4YjYL+lmSUta1yaAZo0bdtuWdIukLRFx3ajl80Y97QJJ5ek8AXTURL6NXyrpK5Kesr2ptuwqSctsL5YUkrZJ+lpLOgRQiYl8G/8zSWON2xXH1AF0F86gA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJDHuT0lXujH7FUn/M2rRbEm72tbA+9OtvXVrXxK9NarK3o6KiI+MVWhr2N+zcbsvIno71kBBt/bWrX1J9NaodvXGYTyQBGEHkuh02Fd3ePsl3dpbt/Yl0Vuj2tJbRz+zA2ifTu/ZAbQJYQeS6EjYbZ9j+xnbz9u+shM91GN7m+2nbG+y3dfhXtbY3ml786hlPbbX236udjvmHHsd6u0a2y/X3rtNts/tUG/zbT9oe4vtp21/u7a8o+9doa+2vG9t/8xue4qkZyV9VtJ2SY9KWhYRv2hrI3XY3iapNyI6fgKG7dMlvSnptog4obbsHyUNRsSq2j+UsyLiii7p7RpJb3Z6Gu/abEXzRk8zLul8SV9VB9+7Ql9fVBvet07s2ZdIej4itkbEXkl3STqvA310vYh4WNLguxafJ2lt7f5ajfzP0nZ1eusKEdEfEY/X7r8h6cA04x197wp9tUUnwn6kpF+Nerxd3TXfe0j6ie3HbC/vdDNjmBsR/dLI/zyS5nS4n3cbdxrvdnrXNONd8941Mv15szoR9rGmkuqm8b+lEfEZSZ+TdFntcBUTM6FpvNtljGnGu0Kj0583qxNh3y5p/qjHH5e0owN9jCkidtRud0q6T903FfXAgRl0a7c7O9zP/+umabzHmmZcXfDedXL6806E/VFJi2wvsH2IpC9JWteBPt7D9ozaFyeyPUPS2eq+qajXSbq4dv9iSfd3sJd36JZpvOtNM64Ov3cdn/48Itr+J+lcjXwj/4Kkv+xED3X6+oSkJ2p/T3e6N0l3auSwbkgjR0SXSvqwpA2Snqvd9nRRb/8u6SlJT2okWPM61NtpGvlo+KSkTbW/czv93hX6asv7xumyQBKcQQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfs4RxaLJFjqkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_train[0, :, :]) # пример данных\n",
    "plt.show()\n",
    "print(y_train[0]) # такргетное значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape([-1, 28 * 28]) # переводим трёхмерный тензор в двумерный\n",
    "X_test = x_test.reshape([-1, 28 * 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве функции потерь используем перектрестную энтропию в многоклассовом варианте:\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = \\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$, то есть вероятность принадлежать классу, которую предсказывает наша модель. В векторном виде $\\vec{y^T}  \\ln \\vec{\\hat{y}}$. Градиент по входному значению $ \\vec{\\hat{y}} $ равен $ \\vec{y} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        loss_value = 0.0\n",
    "        self.state = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        loss_value = y_true.dot(np.log(y_true))[0]\n",
    "                \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу y_pred\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента y_pred\n",
    "        \n",
    "        partial_grad = np.zeros_like(y_pred)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = y_true\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот класс мы прописали в предыдущем задании. Только теперь в backward реализуем абстрактно от входного вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = y_in\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert X.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "            self.theta = np.random.normal(scale=0.01, size = (X.shape[1], 1))\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём функцию активации `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Activation, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # y_pred = ...\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве функции активации $\\Psi$ используем `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "Как описано в условии задания `softmax` не численно стабильный и нужно рассмотреть случаи, когда показатель экспоненты либо очень большое число, или наоборот достаточно маленькое.\n",
    "\n",
    "Для защиты от этого воспользуемся свойством `softmax`, что $softmax(x) = softmax(x+c)$ для любой константы $c$. Следовательно, можем вычисть максимальный элемент в исходном векторе из всех элементов, то есть для входного вектора $x$ определим $z$ так, чтобы: $z = x - \\max(x) $ и затем будем брать `softmax` от вектора z, который будет стабильным "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем градиент от `softmax`: \n",
    "\n",
    "$$\\begin{equation}\\frac{\\partial \\operatorname{smax}\\left(x_{i}\\right)}{\\partial x_{i}}=\\frac{f^{\\prime}(x) g(x)-f(x) g^{\\prime}(x)}{g(x)^{2}}\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation}=\\frac{e^{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}-e^{x_{i}} \\frac{\\partial}{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}}{\\left(\\sum_{j=1}^{|X|} e^{x_{j}}\\right)^{2}}\\end{equation} $$\n",
    "\n",
    "$$\\begin{equation}=\\frac{e^{x_{i}} \\sum_{j=1}^{|X|} e^{x_{j}}-\\left(e^{x_{i}}\\right)^{2}}{\\left(\\sum_{j=1}^{|X|} e^{x_{j}}\\right)^{2}}\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation}=\\left(\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}} \\frac{\\sum_{j=1}^{|X|} e^{x_{j}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)-\\left(\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}} \\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)\\end{equation}$$\n",
    "\n",
    "$$ \\begin{equation}=\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\left(1-\\frac{e^{x_{i}}}{\\sum_{j=1}^{|X|} e^{x_{j}}}\\right)\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}=\\operatorname{smax}\\left(x_{i}\\right)\\left(1-\\operatorname{smax}\\left(x_{i}\\right)\\right)\\end{equation} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В numpy: $smax(\\vec{x}) \\times (1 - smax(\\vec{x}))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, y_in):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = Softmax(y_in) * (1 - Softmax(y_in))\n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, y_in):  \n",
    "        y_pred = np.zeros_like(y_in)\n",
    "        \n",
    "        z = y_in - np.max(y_in) #для того чтобы сделать softmax численно стабильным\n",
    "        denominator = 0.0\n",
    "        for el in z:\n",
    "            denominator += np.exp(el)\n",
    "\n",
    "        y_pred = [np.exp(a) / denominator for a in z]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.modules = None\n",
    "        self.parameters = None\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # y_pred = ...\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(X, y, model, loss_fn, epochs=100):\n",
    "    loss_history = []\n",
    "    pbar = tqdm(total=epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # В этом цикле следует реализовать итеративную процедуру оптимизации параметров модели model,\n",
    "        #        руководствуясь функцией потерь loss_fn\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # loss_value = ...\n",
    "        # grad = ...\n",
    "        # model.theta = ...\n",
    "        \n",
    "        loss_history.append(loss_value)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'loss': loss_value})\n",
    "    pbar.close()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fn = loss()\n",
    "model = Perceptron()\n",
    "loss_history = train_loop(Xtr, ytr, lr_model, obj_fn, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
